# QEM-Former Presentation Slides
## Team 15 - Hack the Horizon Hackathon

---

# SLIDE 1: TITLE

**Data-Driven Quantum Error Mitigation**
**AI-Assisted Graph Transformer Approach**

üî¨ Team 15

- **Nakahosa Dinovic** - Resources Research, Reporter
- **Favour Idowu** - Validation Reviewer, Debugger  
- **Abdulmalek Baitulmal** - Mentor, Solutions Integration

*Hack the Horizon Hackathon - African Quantum Consortium*

---

# SLIDE 2: THE PROBLEM

## NISQ Devices Are Noisy

```
Ideal:  ‚ü®O‚ü© = Tr[OœÅ]
Noisy:  ‚ü®O‚ü© = Tr[O¬∑ùí©(œÅ)]  ‚Üê Corrupted!
```

**Error Sources:**
- üå°Ô∏è Thermal relaxation (T1/T2 decay)
- üìä Readout errors
- üîó Crosstalk between qubits

**Our Goal:**
> Learn a function f_Œ∏ that maps noisy ‚Üí ideal

---

# SLIDE 3: OUR SOLUTION OVERVIEW

## QEM-Former Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CDR + Pauli    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   QEM-Former     ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ   Benchmarking  ‚îÇ
‚îÇ  Data Generation‚îÇ     ‚îÇ  Graph Transformer‚îÇ     ‚îÇ   & Validation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Three Key Innovations:**
1. üìä **CDR** - Efficient training data generation
2. üîÄ **Pauli Twirling** - Stochastic noise conversion
3. üï∏Ô∏è **Graph Transformer** - Topology-aware architecture

---

# SLIDE 4: DATA GENERATION

## Clifford Data Regression (CDR)

**The Problem:** Computing ideal values costs O(2^n) - exponential!

**The Solution:** Clifford circuits simulate in polynomial time

```python
# CDR Algorithm
for i in range(N_samples):
    circuit = RandomCliffordCircuit(n_qubits, depth)
    circuit_twirled = PauliTwirl(circuit)
    y_ideal = StabilizerSimulation(circuit)  # O(poly(n)) ‚úì
    x_noisy = NoisySimulation(circuit_twirled)
    save(circuit ‚Üí Graph, x_noisy, y_ideal)
```

**Dataset Statistics:**
| Component | Samples | Circuit Type |
|-----------|---------|--------------|
| Clifford | 5,010 | Random Clifford |
| Mixed | 2,000 | 60% Clifford, 20% QAOA, 20% VQE |
| **Total** | **7,010** | Mixed |

---

# SLIDE 5: PAULI TWIRLING

## Converting Coherent ‚Üí Stochastic Errors

**Before Twirling:**
```
CNOT ‚Üí Coherent over-rotation (hard to learn)
```

**After Twirling:**
```
P_c ‚äó P_t ¬∑ CNOT ¬∑ P'_c ‚äó P'_t ‚Üí Stochastic Pauli channel (learnable!)
```

Where P ‚àà {I, X, Y, Z} are random Paulis

**Why It Matters:**
- Neural networks learn stochastic patterns better
- Noise becomes predictable across ensemble

---

# SLIDE 6: QEM-FORMER ARCHITECTURE

## Graph Transformer for Quantum Circuits

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     QEM-Former Architecture                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ Circuit ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Node Embed   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇTransformerConv‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  DAG    ‚îÇ    ‚îÇ (64-dim)     ‚îÇ    ‚îÇ   (√ó2)      ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                            ‚îÇ                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ  ‚ñº                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Global Pool   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Context Fusion‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ MLP Head    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ    ‚îÇ               ‚îÇ    ‚îÇ (128‚Üí64‚Üí1)  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                               ‚îÇ                    ‚îÇ             ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚ñº             ‚îÇ
‚îÇ                    ‚îÇ [z0_noisy, zz_noisy,‚îÇ    ‚ü®Z‚ÇÄ‚ü©_ideal        ‚îÇ
‚îÇ                    ‚îÇ  n_qubits, depth,   ‚îÇ                       ‚îÇ
‚îÇ                    ‚îÇ  noise_scale]       ‚îÇ                       ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why Graphs?**
- Circuits ARE Directed Acyclic Graphs
- Nodes = Gates (H, CNOT, RZ...)
- Edges = Qubit wire connections

---

# SLIDE 7: NOISE MODEL

## Thermal Relaxation + Readout Errors

**T1/T2 Relaxation:**
```
p_reset = 1 - e^(-t_g/T1)
p_z = ¬Ω(1 - e^(-t_g/T2))(1 - p_reset)
```

**Our Parameters:**
| Parameter | Value |
|-----------|-------|
| T1 | 50 Œºs (baseline) |
| T2 | 70 Œºs (baseline) |
| 1-qubit gate | 50 ns |
| 2-qubit gate | 400 ns |
| Readout error | 5% |

**Why Simulated (Not IBM Hardware)?**
‚úÖ Reproducibility  
‚úÖ Controlled experiments  
‚úÖ Exact ground truth via statevector

---

# SLIDE 8: TRAINING

## Hyperparameters & Convergence

| Parameter | Value |
|-----------|-------|
| Optimizer | Adam |
| Learning Rate | 0.001 ‚Üí 0.000004 |
| LR Scheduler | ReduceLROnPlateau |
| Batch Size | 32 |
| Epochs | 100 |
| Best Epoch | 22 |

**Training Dynamics:**
- Loss: 0.24 ‚Üí 0.02 in first 10 epochs
- Best validation loss: **0.0094**
- LR decayed 5 times
- No catastrophic overfitting

---

# SLIDE 9: RESULTS - SUCCESS

## Benchmark Results: What Worked

| Circuit Type | Win Rate | Error Reduction | Mean IR |
|--------------|----------|-----------------|---------|
| **Variational** | **80%** ‚úÖ | **31.9%** | 1.44x |
| **Clifford** | 66.7% ‚úÖ | 31.2% | 1.40x |

**Key Metrics:**
- **Win Rate**: How often QEM-Former beats raw noisy measurement
- **Improvement Ratio**: Error_noisy / Error_QEM (>1 is good)

**Interpretation:**
> Our model successfully generalizes from mostly-Clifford training to unseen Variational circuits!

---

# SLIDE 10: RESULTS - FAILURE

## Honest Failure Analysis: QAOA

| Circuit Type | Win Rate | Error Reduction |
|--------------|----------|-----------------|
| QAOA | **15%** ‚ùå | **-115%** |

**Root Cause:**
- QAOA ideal values ‚âà 0
- Model trained on Clifford (values = ¬±1)
- Model "corrects" toward ¬±0.05, increasing error

**Proposed Solutions:**
1. Increase QAOA training proportion (8% ‚Üí 30%)
2. Add circuit-type embedding
3. Separate prediction heads by observable range
4. Uncertainty quantification to abstain on low-confidence

---

# SLIDE 11: ARCHITECTURE EVOLUTION

## Why QEM-Former Was the Right Choice

| Model | Val MSE | Topology-Aware | Noise-Aware |
|-------|---------|----------------|-------------|
| SVR (baseline) | 0.03 | ‚ùå | ‚ùå |
| LSTM | 0.03 | Partial | ‚ùå |
| GCN | 0.02 | ‚úÖ | ‚ùå |
| **QEM-Former** | **0.009** | ‚úÖ | ‚úÖ |

**Result:** QEM-Former achieves **3.3x better MSE** than baselines

**Key Insight:**
> Circuit topology matters. MLPs and LSTMs ignore it; Graph Transformers capture it.

---

# SLIDE 12: DEMO PREVIEW

## Interactive Streamlit Dashboard

**Features:**
- üìà View benchmark results
- ‚öôÔ∏è Adjust noise parameters
- üîÑ Real-time predictions
- üìä Compare QEM vs ZNE vs Noisy

```bash
# Launch the demo
streamlit run dashboard.py
```

*[Live demo follows]*

---

# SLIDE 13: IMPACT & SCALABILITY

## Beyond the Hackathon

**Scalability Potential:**
- Graph representation ‚Üí extensible to 100+ qubits
- Transfer learning from small circuits
- Circuit knitting for distributed simulation

**Industry Relevance:**
- Complements hardware error correction
- No additional quantum resources needed
- Applicable to IBM, Google, Amazon quantum systems

**African Development:**
- Quantum technology: frontier for African leadership
- AI + Quantum = high-impact intersection
- This work: proof African teams can contribute cutting-edge research

---

# SLIDE 14: SUMMARY

## Key Contributions

1. ‚úÖ **Complete Pipeline** - CDR + Pauli Twirling + QEM-Former
2. ‚úÖ **31.9% Error Reduction** on Variational circuits
3. ‚úÖ **80% Win Rate** vs noisy baselines
4. ‚úÖ **Transparent Failure Analysis** - QAOA limitations documented
5. ‚úÖ **Reproducible Codebase** with Streamlit dashboard

**What Makes This Unique:**
- Graph representation of quantum circuits
- Noise context injection
- Multi-observable support
- Honest scientific reporting

---

# SLIDE 15: THANK YOU

## Team 15

- **Nakahosa Dinovic** - Resources Research, Reporter
- **Favour Idowu** - Validation Reviewer, Debugger
- **Abdulmalek Baitulmal** - Mentor, Solutions Integration

**Repository:** [github.com/Abdulmalek-HoM/QEM-Hackathon-Team-15-Repo](https://github.com/Abdulmalek-HoM/QEM-Hackathon-Team-15-Repo)

**Built with AI-Assisted Research** ü§ñ

*Thank you, African Quantum Consortium!*

---

# BACKUP SLIDES

## B1: Q&A - Why Simulated Noise?

**Alternative:** Use real IBM hardware noise

**Our Choice:** Simulated with T1/T2 physics

**Reasons:**
| Factor | Simulated | Hardware |
|--------|-----------|----------|
| Reproducibility | ‚úÖ Perfect | Varies daily |
| Ground Truth | ‚úÖ Statevector | Needs tomography |
| Scientific Control | ‚úÖ Isolated | Coupled effects |

---

## B2: Q&A - How to Scale to 100+ Qubits?

1. **Circuit Knitting** - Distribute simulation
2. **Tensor Networks** - Approximate ground truth
3. **Transfer Learning** - Train on small, apply to large
4. **Approximate Methods** - MPS, DMRG for validation

---

## B3: Code Snippets

**Data Generation:**
```python
python data_gen_advanced.py --large
python data_gen_advanced.py --mixed --samples 2000
```

**Training:**
```python
python train_qem.py
```

**Benchmarking:**
```python
python benchmark_suite.py
```
